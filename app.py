"""
æ ªä¸»å„ªå¾… åº—èˆ—æ¤œç´¢ v10
- é§…åãƒ»ã‚¨ãƒªã‚¢å…¥åŠ› â†’ ä½æ‰€ä¸€è‡´ã§çµã‚Šè¾¼ã¿
- çµã‚Šè¾¼ã¿çµæœã‚’ã€ŒGoogleãƒãƒƒãƒ—ã§ä¸€æ‹¬æ¤œç´¢ã€ãƒœã‚¿ãƒ³ã§é–‹ã
- å„åº—èˆ—ã«ã‚‚å€‹åˆ¥Googleãƒãƒƒãƒ—ãƒªãƒ³ã‚¯
- Nominatim/åº§æ¨™å¤‰æ›ãªã—ãƒ»å®Œå…¨ç„¡æ–™
"""
import streamlit as st
import pandas as pd
import urllib.parse
from pathlib import Path
from pdf_parser import extract_stores_from_pdf

st.set_page_config(page_title="æ ªä¸»å„ªå¾… åº—èˆ—æ¤œç´¢", page_icon="ğŸ«", layout="wide")

st.markdown("""
<style>
@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@400;700&display=swap');
* { font-family: 'Noto Sans JP', sans-serif; }
.store-card {
    background: #fff;
    border: 1px solid #e0e0e0;
    border-left: 5px solid #1a73e8;
    border-radius: 6px;
    padding: 10px 14px;
    margin: 4px 0;
}
.store-name { font-size: 1em; font-weight: 700; color: #202124; }
.store-addr { font-size: 0.85em; color: #444; margin-top: 2px; }
.store-tel  { font-size: 0.82em; color: #888; }
</style>
""", unsafe_allow_html=True)

st.title("ğŸ« æ ªä¸»å„ªå¾… åº—èˆ—æ¤œç´¢")

# â”€â”€ ã‚µã‚¤ãƒ‰ãƒãƒ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar:
    st.header("ğŸ“‚ PDFèª­ã¿è¾¼ã¿")
    uploaded = st.file_uploader("æ ªä¸»å„ªå¾…PDF", type=["pdf"], accept_multiple_files=True)

    pdf_dir = Path("pdfs")
    preloaded = sorted(pdf_dir.glob("*.pdf")) if pdf_dir.exists() else []
    if preloaded:
        st.caption(f"ğŸ“ pdfs/ ã‹ã‚‰ {len(preloaded)} ä»¶ã‚’è‡ªå‹•èª­ã¿è¾¼ã¿")

    st.divider()
    st.header("ğŸ” ã‚¨ãƒªã‚¢æ¤œç´¢")
    keyword = st.text_input(
        "é§…åãƒ»å¸‚åŒºç”ºæ‘",
        placeholder="ä¾‹ï¼šæ¨ªæµœã€æ–°å®¿ã€å·å´å¸‚",
    )

# â”€â”€ PDFè§£æ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class _FileProxy:
    def __init__(self, p):
        self.name = p.name
        self._data = p.read_bytes()
        self._pos = 0
    def read(self, n=-1):
        d = self._data[self._pos:]; self._pos = len(self._data); return d
    def seek(self, p): self._pos = p

sources = list(uploaded or []) + [_FileProxy(p) for p in preloaded]

if not sources:
    st.info("ğŸ‘† ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
    st.stop()

all_stores = []
for src in sources:
    try:
        all_stores.extend(extract_stores_from_pdf(src, source_type="upload"))
    except Exception as e:
        st.error(f"âŒ {src.name}: {e}")

if not all_stores:
    st.error("åº—èˆ—æƒ…å ±ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
    st.stop()

# â”€â”€ éƒ½é“åºœçœŒãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
all_prefs = sorted(set(s.get("pref", "") for s in all_stores if s.get("pref")))
with st.sidebar:
    selected_prefs = st.multiselect("éƒ½é“åºœçœŒã§çµã‚Šè¾¼ã¿", all_prefs, default=all_prefs)

# â”€â”€ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰çµã‚Šè¾¼ã¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
filtered = [s for s in all_stores if s.get("pref") in selected_prefs]

kw = keyword.strip()
if kw:
    filtered = [
        s for s in filtered
        if kw in s.get("address", "") or kw in s.get("name", "")
    ]

# â”€â”€ çµæœãƒ˜ãƒƒãƒ€ãƒ¼ï¼‹ä¸€æ‹¬Googleãƒãƒƒãƒ—ãƒœã‚¿ãƒ³ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown(f"**{len(all_stores)}ä»¶ä¸­ {len(filtered)}ä»¶**ã‚’è¡¨ç¤º" +
            (f"ï¼ˆã€Œ{kw}ã€ã§çµã‚Šè¾¼ã¿ï¼‰" if kw else ""))

if filtered and kw:
    # çµã‚Šè¾¼ã¿çµæœã®ä½æ‰€ã‚’ã€Œ/ã€ã§é€£çµã—ã¦Googleãƒãƒƒãƒ—æ¤œç´¢
    # â€» å¤šã™ãã‚‹ã¨URLãŒé•·ããªã‚‹ã®ã§æœ€å¤§15ä»¶
    addrs = [s.get("address", "") for s in filtered[:15]]
    # Googleãƒãƒƒãƒ—ã®è¤‡æ•°åœ°ç‚¹æ¤œç´¢ï¼šä½æ‰€ã‚’ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§ã¾ã¨ã‚ã¦æ¤œç´¢
    # å®Ÿç”¨çš„ã«ã¯ã€Œã‚¨ãƒªã‚¢å + æœ€åˆã®æ•°åº—èˆ—åã€ã§æ¤œç´¢ã™ã‚‹ã®ãŒè¦‹ã‚„ã™ã„
    store_names = "ã€€".join(s.get("name", "") for s in filtered[:8])
    bulk_query = urllib.parse.quote(f"{kw} {store_names}")
    bulk_url = f"https://www.google.com/maps/search/{bulk_query}"

    # ã‚·ãƒ³ãƒ—ãƒ«ã«ã€Œé§…åã€ã ã‘ã§æ¤œç´¢ã—ã¦ãã®å‘¨è¾ºã‚’è¦‹ã›ã‚‹æ–¹ãŒå®Ÿç”¨çš„
    area_url = f"https://www.google.com/maps/search/{urllib.parse.quote(kw)}"

    col1, col2 = st.columns(2)
    with col1:
        st.link_button(
            f"ğŸ—ºï¸ ã€Œ{kw}ã€ã‚’Googleãƒãƒƒãƒ—ã§é–‹ãï¼ˆå‘¨è¾ºç¢ºèªï¼‰",
            area_url,
            use_container_width=True,
        )
    with col2:
        # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        df_dl = pd.DataFrame([{
            "åº—èˆ—å": s["name"], "ä½æ‰€": s["address"], "é›»è©±": s.get("tel",""),
            "GoogleMap": "https://www.google.com/maps/search/?api=1&query=" + urllib.parse.quote(s["address"])
        } for s in filtered])
        st.download_button(
            "â¬‡ï¸ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆGoogleãƒã‚¤ãƒãƒƒãƒ—ç”¨ï¼‰",
            df_dl.to_csv(index=False).encode("utf-8-sig"),
            file_name=f"å„ªå¾…_{kw or 'å…¨ä»¶'}.csv", mime="text/csv",
            use_container_width=True,
        )

if not filtered:
    st.warning("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹åº—èˆ—ãŒã‚ã‚Šã¾ã›ã‚“")
    st.stop()

st.divider()

# â”€â”€ åº—èˆ—ãƒªã‚¹ãƒˆï¼ˆéƒ½é“åºœçœŒã‚°ãƒ«ãƒ¼ãƒ—è¡¨ç¤ºï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
pref_order = {p: i for i, p in enumerate(all_prefs)}
filtered_sorted = sorted(filtered, key=lambda s: (pref_order.get(s.get("pref",""), 999), s.get("address","")))

cur_pref = None
for s in filtered_sorted:
    pref = s.get("pref", "")
    if pref != cur_pref:
        cur_pref = pref
        st.subheader(f"ğŸ“ {pref}")

    addr = s.get("address", "")
    gmap_url = "https://www.google.com/maps/search/?api=1&query=" + urllib.parse.quote(addr)

    c1, c2 = st.columns([6, 1])
    with c1:
        st.markdown(
            f'<div class="store-card">'
            f'<div class="store-name">{s.get("name","")}</div>'
            f'<div class="store-addr">ğŸ“® {addr}</div>'
            f'<div class="store-tel">ğŸ“ {s.get("tel","")}</div>'
            f'</div>',
            unsafe_allow_html=True,
        )
    with c2:
        st.link_button("ğŸ—ºï¸ åœ°å›³", gmap_url, use_container_width=True)
